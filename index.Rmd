---
title: "Predicting Exercise Manner"
author: "Jeff Olson"
date: "3/29/2021"
output: html_document
---
# Assignment
Predict the manner in which people exercise using data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants. Six healthy participants (20-28 years old, with little weight lifting experience) were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). (See section on Weight Lifting Exercise Dataset, Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H., http://groupware.les.inf.puc-rio.br/har).  
```{r echo=FALSE, cache=TRUE, include=FALSE}
# libraries
packages = c("tidyverse",
             "caret",
             "e1071",
             "doParallel",
             "parallel")
package.check <- lapply(packages, FUN = function(x) {
        if (!require(x, character.only = TRUE)) {
                install.packages(x, dependencies = TRUE)
                library(x, character.only = TRUE)
        }
})

# Download and read training and testing data and create validation data. 
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
              destfile = "training.csv")
training <- read.csv("training.csv")
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
              destfile = "testing.csv")
testing <- read.csv("testing.csv")
length_var_train <- vector(length = 160)
length_var_test <- vector(length = 160)
for(i in 1:160) {
        length_var_train[i] <- length(unique(training[,i]))
}
for(i in 1:160) {
        length_var_test[i] <- length(unique(testing[,i]))
}
var_one_train <- unique(names(training[,length_var_train <= 1]))
var_one_test <- unique(names(testing[,length_var_test <= 1]))
var_one_all <- unique(c(var_one_train, var_one_test))
training <- training %>%
        select(-all_of(var_one_all))
testing <- testing %>%
        select(-all_of(var_one_all)) 
set.seed(39880)
inTrain = createDataPartition(training$classe, p = 1/2)[[1]]
valid = training[ inTrain,]
train = training[ -inTrain,]
rm(package.check,
   i,
   inTrain,
   length_var_test,
   length_var_train,
   packages, 
   var_one_all,
   var_one_test,
   var_one_train)
# Run model
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
fitControl <- trainControl(method = "cv",
                           number = 5,
                           allowParallel = TRUE)
mtry <- 4
tunegrid <- expand.grid(.mtry = mtry)
set.seed(39880)
system.time(fit <- train(as.factor(classe) ~ ., 
                         data = train, 
                         method    = "rf",
                         trContol = fitControl, 
                         tuneGrid = tunegrid))
stopCluster(cluster)
registerDoSEQ()
```
# Results  
Using a random forest approach in the caret package of R, I was able to develop an algorithm that predicted almost 100% of the exercise manners in six cross-validations of the training data, using the random forest method, with cross-validation. I applied it to a validation set that was almost as accurate and then to the test data, with 100% accuracy.  

# How the Model Was Built  
The random forest method was chosen, because it has shown itself to be effective in predicting classifications between more than two qualitative outcomes, as in this case. It tries multiple combinations of variables and selects combinations based on predictive value.  

# Variable Selection  
The initial choice of variables was based on eliminating those with no variance. With only one value, they could not provide any discrimination (although, perhaps, NA observations might have been used as a second value, a possibility for future consideration). These variables were identified and removed from both the training and testing data sets, reducing the number of variables from 160 to 59. Caret provides for pre-processing methods that address this type of issue, but applicable ones for eliminating variables with little or no variance, such as "zv" and "nzv", only apply to numeric variables. Most of the problematic variables were not numeric, so they needed to be eliminated in a pre-process that was not possible through the function preProcess. The remaining variables were almost entirely variables that were direct measurements of the movements.  
```{r echo=FALSE, cache=TRUE}
varImp(fit)
```
To avoid overfitting, mtry was reduced sequentially from the square root of the number of predictors, the default, until the accuracy started to diminish.  

**Final Model**  
```{r echo=FALSE, cache=TRUE}
fit$finalModel
```
# Cross Validation  
Cross validation is at the heart of the random forest method. According to the creators, "In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run, as follows: Each tree is constructed using a different bootstrap sample from the original data. About one-third of the cases are left out of the bootstrap sample and not used in the construction of the kth tree."(Breiman and Cutler, Random Forests, https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm).
The confusion matrix provides a cross-validated (5 fold) set of results.  

**Confusion Matrix**  
```{r echo=FALSE, cache=TRUE}
confusionMatrix.train(fit)
```
Half of the training set was set aside as a validation set. Here are the results associated with the confustion matrix of the validation set:    
  
**Confusion matrix of the validation set**  
```{r echo=FALSE, cache=TRUE}
predValid <- predict(fit, valid)
table(predValid)
confusionMatrix(predValid, as.factor(valid$classe))
```
# Out-of-Sample Error  
The out-of-sample error is the error associated with the testing data. There was no error associated with the testing data. All 20 predictions matched the testing outcomes. 
